---
title: "Kaggle Competition:Predicting Online News Popularity"
author: "Team 3-NN: Kseniya Bout, Balint Van, Leonardo Byon"
output: pdf_document
---
#ABSTRACT
text to be included later

#INTRODUCTION
We were provided a dataset of online news articles published by Mashable.com with the goal of developing a multi-class classifiers for predicting the level of popularity for each article. The dataset is composed by 60 features and consists of 39,000 articles, of which 30,000 are labeled training data, with the remainder serving as a test set. The popularity is a function of the number of times the article has been shared, where total number of classes is five, where (1) are those that were shared a few times, while (5) being those that went viral. The evaluation critiera is  Classification Accuracy. 

The current report describes the approaches taken by our team, 3-NN, to tackle the competition. The report is organized as follows: ...................

#EXPLORATORY ANALYSIS

##Principal Component Analysis (PCA)
PCA helped us gain better understanding on the relevance of features, where those closely associated with popularity were average number of key words and news category/topic. For example, there is positive correlation between popularity and the number of average key words and LDA_03 (undiscosed topic). Additionally, low popularity is related with World and Enterntainment news, while mid levels of popularity are associated with Technology-related news. 

[leo/#images/PCA_combined.png]
[leo/#images/PCA_Observations.png]

##Variable Importance
To cross check our interpretations on variable importance of the PCA above, and to, additionally, understand how variable importance is determined by popular off-the-shelf ensemble methods, we implemented standard Random Forest (RF) and Gradient Boosting Machines (GBM). On one hand, RF decorrelated bagged trees by randomly selecting M predictors from the full set of P predictors for each tree at each iteration, where each tree is independent of the other trees, while on the other, GBM grows low-depth trees sequentially, by using error information obtained from previously grown trees. Results are displayed below, sorted by importance as per RF. Although significantly different, top features are similar with each method and consistent with PCA interpretations. We also notice that the regularization process of GBM is much more strict than that of RF, which suggests RF having higher potential overfitting risk.

[leo/#images/VarImp_comparison.png]

##Class Imbalance and Positive Skewness
In line with the expectations, the distribution of the popularity of online news article, follows a power law, creating high class imbalance is present in the training data, and can naturally expect the same distribution in the test data. 

[make basic frequency table of classes]

In the current competition setup, the cost of Type I/II errors for any class is equal, and therefore we are not concerned with accurately prediction the low frequency of class (4) and (5) articles. Nevertheless, because accurately predicing them could provide a competitive edge, we tried accounting for class in our analysis. Possible strategies include allocating different weights across classes to  increase sensitivity in favor of imbalanced classes, down/up sampling, synthetic minority over-sampling technique (SMOTE), and stratified sampling to equalize the number of obserations across classes. We tried up-sampling and stratified sampling with RF, but were unable to improve over our plain-vanille RF benchmark.

Related to the class imbalance, we observe that many features presented significant positive skewness.

#FEATURE ENGINEERING
To improve algorithmic performance, we applied logarithmic transformations, $log(1+x)$ to the abovementioned skewed features. Additionally, date-related information extracted from the article's URL was discretized.  

According to literature, category of the news, language subjectivity, news source, and frequency and relevance of key words (named entities), have been shown to be good predictors for online news popularity (Bandari et al, 2012). Among these four characteristics, all but the news source and relevance of keyword is provided in the original data set. In our context, we defined news source as the author of each article, which was acquired via web scraping. Out of the entire training and test set, we obtained 340 unique authors and 6200 NULL values, suggesting a high ratio of article to unique authors ratio. We ran standard GBM to identify the most important authors, of which only 37 proved to have predicting power. As a proxy for keyword relevance, we attempted to compute a relevance score using data obtained one week prior to each article's publishing date from Google Trend API, but quota limits were too restrictive to complete the queries.

Nevertheless, the addition of these new features did not help improve our standard RF benchmark.

Posteriorly, we added predictions made by K-NN, as new features for RF, and a slight improvement was observed.


#PREDICTION MODELS AND RESULTS
............


#CONCLUSION
..............

#BIBLIOGRAPHY
Bandari, R., Asur, S., Huberman, B.A.: The pulse of news in social media: Forecasting popularity. CoRR (2012)